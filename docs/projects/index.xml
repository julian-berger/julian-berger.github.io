<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Julian Berger | collective intelligence | wisdom of crowds | machine learning</title>
<link>julian-berger.github.io/projects/</link>
<atom:link href="julian-berger.github.io/projects/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.555</generator>
<lastBuildDate>Thu, 26 Sep 2024 08:31:09 GMT</lastBuildDate>
<item>
  <title>Household debt and repayment behavior</title>
  <link>julian-berger.github.io/projects/debt.html</link>
  <description><![CDATA[ 




<p>We partner with coeo to investigate debtor behavior across 3.4 million debtors in Germany.</p>
<p>Results are work in progress but you can shoot me an email and we talk about behavioral characteristics of people with varying amounts of debt and how to identify early warning signals of repayment struggles.</p>



 ]]></description>
  <guid>julian-berger.github.io/projects/debt.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/debt.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Hybrid Confirmation Trees</title>
  <link>julian-berger.github.io/projects/hct.html</link>
  <description><![CDATA[ 




<p>Abstract: We study hybrid confirmation trees, a simple heuristic for producing hybrid intelligence in melanoma classification. Hybrid confirmation trees first elicit the decision of one human expert and one algorithm. Whenever the two agree a decision is immediately made. In case of disagreement, a second human expert is called in to break the tie. We apply this approach to data on skin cancer detection in dermoscopic and non-dermoscopic images across two studies. Study 1 establishes our approach to be a powerful alternative to human baselines, even outperforming up to three humans combined at lower costs. Study 2 compares hybrid confirmation trees against the industry-standard: humans taking AI advice. Crucially, we find humans supported by AI advice perform worse than hybrid confirmation trees. We attribute this benefit over human-only decision making and AI advice to uncorrelated errors between independent decisions by human and AI. AI advice lead to humans copying AI mistakes while hybrid confirmation trees already exhibit a generally lower agreement on AI mistakes, which can be successfully resolved by human tie breaking. Taken together, our results highlight the potential for combining independent choices of humans and AI for medical diagnostics and beyond as hybrid confirmation trees are a simple but powerful general-purpose method for arbitrating choices.</p>
<p><img src="julian-berger.github.io/images/hct_1.png" class="img-fluid"></p>
<p><img src="julian-berger.github.io/images/hct_vs_advice.png" class="img-fluid"></p>



 ]]></description>
  <guid>julian-berger.github.io/projects/hct.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/hct_vs_advice.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Efficient Crowds</title>
  <link>julian-berger.github.io/projects/crowd_stop.html</link>
  <description><![CDATA[ 




<p>Abstract: Efficiently allocating individuals to work on complex decision problems is a key challenge for groups, organizations and societies, and requires navigating a crucial trade-off: increasing the number of individuals working on a task typically increases accuracy but at the expense of higher costs. Research in collective intelligence has proposed a plethora of mechanisms to pool the judgements of independent decision makers for increasing performance. However, these are all static and do not adjust the number of crowd members to the challenge at hand, resulting in high, fixed costs for every decision problem. We develop and test three decision rules that allow to benefit from the wisdom of the crowd adaptively depending on a case’s difficulty. Our rules rely on decision makers’ confidence judgements for stopping crowd growth. Empirical analyses in four real-world domains (cancer diagnoses, forecasting, false news classification, and criminology) using seven data sets find that our adaptive decision rules can achieve equal (or higher) accuracy compared to widely-used static crowd aggregators while using fewer individuals. Our findings present easily applicable decision guidelines ready to be employed in practice to substantially boost the efficiency of crowds.</p>
<p><img src="julian-berger.github.io/images/crowd_stop_Fig2.png" class="img-fluid"></p>



 ]]></description>
  <guid>julian-berger.github.io/projects/crowd_stop.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/crowd_stop_Fig2.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>LLM-human ensembles</title>
  <link>julian-berger.github.io/projects/LLM_humans.html</link>
  <description><![CDATA[ 




<p>Abstract: Artificial intelligence systems, particularly large language models (LLMs), are increasingly being employed in high-stakes decisions that impact both individuals and society at large, often without adequate safeguards to ensure safety, quality, and equity. Yet LLMs hallucinate, lack common sense, and are biased - shortcomings that may reflect LLMs’ inherent limitations and thus may not be remedied by more sophisticated architectures, more data, or more human feedback. Relying solely on LLMs for complex, high-stakes decisions is therefore problematic. Here we present a hybrid collective intelligence system that mitigates these risks by leveraging the complementary strengths of human experience and the vast information processed by LLMs. We apply our method to open-ended medical diagnostics, combining 40,762 differential diagnoses made by physicians with the diagnoses of five state-of-the art LLMs across 2,133 medical cases. We show that hybrid collectives of physicians and LLMs outperform both single physicians and physician collectives, as well as single LLMs and LLM ensembles. This result holds across a range of medical specialties and professional experience, and can be attributed to humans’ and LLMs’ complementary contributions that lead to different kinds of errors. Our approach highlights the potential for collective human and machine intelligence to improve accuracy in complex, open-ended domains like medical diagnostics.</p>
<p><a href="https://arxiv.org/abs/2406.14981">PREPRINT HERE</a></p>



 ]]></description>
  <guid>julian-berger.github.io/projects/LLM_humans.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/complementarity_physicians_LLMs.png" medium="image" type="image/png" height="140" width="144"/>
</item>
<item>
  <title>How large language models can reshape collective intelligence</title>
  <link>julian-berger.github.io/projects/LLMxCI.html</link>
  <description><![CDATA[ 




<p>Abstract: Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.</p>
<p><a href="https://www.nature.com/articles/s41562-024-01959-9">LINK</a></p>
<p>Citation: Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., <strong>Berger, J.,</strong> … &amp; Hertwig, R. (2024). How large language models can reshape collective intelligence. <em>Nature Human Behaviour</em>.</p>



 ]]></description>
  <guid>julian-berger.github.io/projects/LLMxCI.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/LLMxCI.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Interpretable Credit Scores</title>
  <link>julian-berger.github.io/projects/credit.html</link>
  <description><![CDATA[ 




<p>This project evaluated several credit scores in the lab and in the field WITH consumers. The scores we built to be accurate, fair and transparent.</p>
<p>This project is in cooperation with Germany’s largest credit scoring agency. Due to NDA and ongoing developments, public announcements and pre-prints of the research is not to be expected before May 2025.</p>
<p>If you want to know more, shoot me an email and we can have a chat.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="julian-berger.github.io/images/credit.webp" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>



 ]]></description>
  <guid>julian-berger.github.io/projects/credit.html</guid>
  <pubDate>Thu, 26 Sep 2024 08:31:09 GMT</pubDate>
  <media:content url="julian-berger.github.io/images/credit.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
